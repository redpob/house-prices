{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a49b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "# Read data\n",
    "train = pd.read_csv('AmesHousing.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train2 = pd.read_csv('train.csv')\n",
    "train.columns = train.columns.str.replace(' ', '')\n",
    "train = train.rename(columns = {\"YearRemod/Add\": \"YearRemodAdd\"}) # keep variables' name constant\n",
    "\n",
    "# Remove duplicates in data\n",
    "data = pd.concat([train,train2,test], axis = 0, sort = False)\n",
    "useless = ['Id','PID','Order','SalePrice'] \n",
    "data = data.drop(useless, axis = 1) # only keep features\n",
    "duplicate = data[data.duplicated(keep = 'last')].index\n",
    "duplicate = duplicate[0:390]\n",
    "train = train.drop(duplicate, axis = 0)\n",
    "training = pd.concat([train,train2], axis = 0, sort = False)\n",
    "useless = ['Id','PID','Order'] \n",
    "training = training.drop(useless, axis = 1) # final training dataset\n",
    "\n",
    "# Separating Target and Features\n",
    "target = training['SalePrice']\n",
    "test_id = test['Id']\n",
    "test = test.drop(['Id'],axis = 1)\n",
    "training2 = training.drop(['SalePrice'], axis = 1)\n",
    "\n",
    "# Concatenating train & test set\n",
    "train_test = pd.concat([training2,test], axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c3a465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting non-numeric predictors stored as numbers into string\n",
    "train_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\n",
    "train_test['YrSold'] = train_test['YrSold'].apply(str)\n",
    "train_test['MoSold'] = train_test['MoSold'].apply(str)\n",
    "train_test['OverallQual'] = train_test['OverallQual'].apply(str)\n",
    "train_test['OverallCond'] = train_test['OverallCond'].apply(str)\n",
    "\n",
    "# Fill missing values\n",
    "# Categorical features\n",
    "# Fill nan based on the description file \n",
    "train_test['Functional'] = train_test['Functional'].fillna('Typ')\n",
    "train_test['Electrical'] = train_test['Electrical'].fillna(\"SBrkr\")\n",
    "train_test['KitchenQual'] = train_test['KitchenQual'].fillna(\"TA\")\n",
    "\n",
    "# Fill nan with most frequnt class\n",
    "train_test['Exterior1st'] = train_test['Exterior1st'].fillna(train_test['Exterior1st'].mode()[0])\n",
    "train_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(train_test['Exterior2nd'].mode()[0])\n",
    "train_test['SaleType'] = train_test['SaleType'].fillna(train_test['SaleType'].mode()[0])\n",
    "\n",
    "# Fill nan with none\n",
    "train_test[\"PoolQC\"] = train_test[\"PoolQC\"].fillna(\"None\")\n",
    "train_test[\"Alley\"] = train_test[\"Alley\"].fillna(\"None\")\n",
    "train_test['FireplaceQu'] = train_test['FireplaceQu'].fillna(\"None\")\n",
    "train_test['Fence'] = train_test['Fence'].fillna(\"None\")\n",
    "train_test['MiscFeature'] = train_test['MiscFeature'].fillna(\"None\")\n",
    "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    train_test[col] = train_test[col].fillna('None')\n",
    "    \n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    train_test[col] = train_test[col].fillna('None')\n",
    "    \n",
    "# Numeric features   \n",
    "# Fill nan with zero\n",
    "for col in ('GarageArea', 'GarageCars'):\n",
    "    train_test[col] = train_test[col].fillna(0)\n",
    "        \n",
    "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea','BsmtUnfSF', 'TotalBsmtSF'):\n",
    "    train_test[col] = train_test[col].fillna(0)\n",
    "    \n",
    "# Fill nan with median vlues \n",
    "train_test['LotFrontage'] = train_test['LotFrontage'].fillna(train['LotFrontage'].median())\n",
    "\n",
    "# Add new features based on common knowledge\n",
    "train_test[\"SqFtPerRoom\"] = train_test[\"GrLivArea\"] / (train_test[\"TotRmsAbvGrd\"] +\n",
    "                                                       train_test[\"FullBath\"] +\n",
    "                                                       train_test[\"HalfBath\"] +\n",
    "                                                       train_test[\"KitchenAbvGr\"])\n",
    "\n",
    "train_test['Total_Home_Quality'] = train_test['OverallQual'] + train_test['OverallCond']\n",
    "\n",
    "train_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n",
    "                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n",
    "\n",
    "train_test[\"HighQualSF\"] = train_test[\"1stFlrSF\"] + train_test[\"2ndFlrSF\"]\n",
    "train_test['renovated']=train_test['YearRemodAdd']+train_test['YearBuilt']\n",
    "\n",
    "# Removing the useless variables\n",
    "useless = ['GarageYrBlt','YearRemodAdd', \n",
    "    'MSSubClass', 'OverallCond', 'BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', \n",
    "    'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', \n",
    "    'MoSold', 'YrSold'\n",
    "          ]\n",
    "train_test = train_test.drop(useless, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aad6dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables from categorical features\n",
    "train_test_dummy = pd.get_dummies(train_test)\n",
    "\n",
    "from scipy.stats import skew\n",
    "numeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index\n",
    "skewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "high_skew = skewed_features[skewed_features > 0.5]\n",
    "skew_index = high_skew.index\n",
    "\n",
    "# Normalize skewed features using log_transformation\n",
    "for i in skew_index:\n",
    "    train_test_dummy[i] = np.log1p(train_test_dummy[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18014029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SalePrice after transformation\n",
    "target_log = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ad6533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_train = train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f2fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test separation\n",
    "X_train = train_test_dummy[0:4000]\n",
    "X_test = train_test_dummy[4000:]\n",
    "\n",
    "# Creation of the RMSE metric:\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def cv_rmse(model):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train, target_log, scoring = \"neg_mean_squared_error\", cv = kf))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ff2e09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0958417839320341\n",
      "0.019434673101199708\n",
      "     Id  SalePrice\n",
      "0  1461   112625.0\n",
      "1  1462   167654.0\n",
      "2  1463   191887.0\n",
      "3  1464   195845.0\n",
      "4  1465   179472.0\n"
     ]
    }
   ],
   "source": [
    "# XGB Regressor\n",
    "xgb = XGBRegressor()\n",
    "kf = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "score_xgb = cv_rmse(xgb)\n",
    "print(score_xgb.mean())\n",
    "print(score_xgb.std())\n",
    "\n",
    "# Train model\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "xgb_model = xgb.fit(X_train, target_log)\n",
    "submission.iloc[:,1] = np.floor(np.expm1(xgb_model.predict(X_test)))\n",
    "print(submission.head())\n",
    "#submission.to_csv(\"xgb_oliver_08041414.csv\", index = False)\n",
    "#0.09156863522810113\n",
    "#0.1230436287439903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5629ba8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_rmse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1c1854b24937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgradientboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore_gradientboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_rmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradientboost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_gradientboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_gradientboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv_rmse' is not defined"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "gradientboost = GradientBoostingRegressor()\n",
    "kf = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "score_gradientboost = cv_rmse(gradientboost)\n",
    "print(score_gradientboost.mean())\n",
    "print(score_gradientboost.std())\n",
    "\n",
    "# Train model\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "gradientboost_model = gradientboost.fit(X_train, target_log)\n",
    "submission.iloc[:,1] = np.floor(np.expm1(gradientboost_model.predict(X_test)))\n",
    "print(submission.head())\n",
    "#submission.to_csv(\"oliver_08061722_gradientboost.csv\", index = False)\n",
    "\n",
    "# RandomizedSearch to improve model\n",
    "para_grid = { 'max_depth': [3,6,10], 'learning_rate': [0.01, 0.05, 0.1],\n",
    "           'n_estimators': [100, 500, 1000], 'subsample': [0.3, 0.7], 'alpha': np.logspace(-9, 0, 10)}\n",
    "grid_search_gradientboost = RandomizedSearchCV(gradientboost, para_grid, scoring = 'neg_mean_squared_error', n_iter = 250, verbose = 1)\n",
    "grid_search_gradientboost.fit(X_train, target_log)\n",
    "print(\"Best parameters:\", grid_search_gradientboost.best_params_)\n",
    "print(\"Lowest RMSE: \", (-grid_search_gradientboost.best_score_)**(1/2.0))\n",
    "submission.iloc[:,1] = np.floor(np.expm1(grid_search_gradientboost.predict(X_test)))\n",
    "print(submission.head())\n",
    "#submission.to_csv(\"oliver_08061722_gradientboost.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7325adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None, gamma=None,\n",
       "                                          gpu_id=None, importance_type='gain',\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=None,\n",
       "                                          n_estimators=100, n_jobs...\n",
       "       6.13590727e+06, 7.39072203e+06, 8.90215085e+06, 1.07226722e+07,\n",
       "       1.29154967e+07, 1.55567614e+07, 1.87381742e+07, 2.25701972e+07,\n",
       "       2.71858824e+07, 3.27454916e+07, 3.94420606e+07, 4.75081016e+07,\n",
       "       5.72236766e+07, 6.89261210e+07, 8.30217568e+07, 1.00000000e+08]),\n",
       "                                        'colsample_bytree': [0.3, 0.7],\n",
       "                                        'learning_rate': [0.01, 0.05, 0.1],\n",
       "                                        'max_depth': [3, 6, 10],\n",
       "                                        'n_estimators': [100, 500, 1000]},\n",
       "                   scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearch/RandomizedSearch to improve the model\n",
    "para_grid = { 'max_depth': [3,6,10], 'learning_rate': [0.01, 0.05, 0.1],\n",
    "           'n_estimators': [100, 500, 1000], 'colsample_bytree': [0.3, 0.7], 'alpha': np.logspace(0, 8, 100)}\n",
    "xgb = XGBRegressor()\n",
    "#grid_search_xgb = GridSearchCV(xgb, para_grid, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "#grid_search_xgb = RandomizedSearchCV(xgb, para_grid, scoring = 'neg_mean_squared_error', n_iter = 250, verbose = 1)\n",
    "grid_search_xgb = RandomizedSearchCV(xgb, para_grid, scoring = 'neg_mean_squared_error', n_iter = 250, verbose = 1)\n",
    "grid_search_xgb.fit(X_train, target_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9845064d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 1000, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.7, 'alpha': 13.530477745798075}\n",
      "Lowest RMSE:  0.1230436287439903\n",
      "     Id  SalePrice\n",
      "0  1461   119191.0\n",
      "1  1462   154002.0\n",
      "2  1463   196317.0\n",
      "3  1464   196199.0\n",
      "4  1465   181732.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters:\", grid_search_xgb.best_params_)\n",
    "print(\"Lowest RMSE: \", (-grid_search_xgb.best_score_)**(1/2.0))\n",
    "submission.iloc[:,1] = np.floor(np.expm1(grid_search_xgb.predict(X_test)))\n",
    "print(submission.head())\n",
    "#submission.to_csv(\"xgb_oliver_08051226.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12456eaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stack = StackingRegressor(regressors = (XGBRegressor(n_estimators = 1000, max_depth = 10, learning_rate = 0.1, colsample_bytree = 0.7, alpha = 13.530477745798075), \n",
    "                                        GradientBoostingRegressor(subsample = 0.3, n_estimators = 500, max_depth = 10, learning_rate = 0.05, alpha = 0.0001)), \n",
    "                          meta_regressor = XGBRegressor(), \n",
    "                          use_features_in_secondary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb06118f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08703126483397647\n",
      "0.020400837690123658\n"
     ]
    }
   ],
   "source": [
    "stack_score = cv_rmse(stack)\n",
    "print(stack_score.mean())\n",
    "print(stack_score.std())\n",
    "# 0.08279143055492903\n",
    "# 0.01901712515315616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdc5fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n",
      "     Id  SalePrice\n",
      "0  1461   107795.0\n",
      "1  1462   167184.0\n",
      "2  1463   191392.0\n",
      "3  1464   192861.0\n",
      "4  1465   186055.0\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "#stack_model = stack.fit(X_train, target_log)\n",
    "#submission.to_csv(\"oliver_08281253_stack.csv\", index = False)\n",
    "\n",
    "# GridSearch/RandomizedSearch to improve the model\n",
    "para_grid = { 'meta_regressor__max_depth': [3,6,10], 'meta_regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "           'meta_regressor__n_estimators': [100, 500, 1000], 'meta_regressor__colsample_bytree': [0.3, 0.7], 'meta_regressor__alpha': np.logspace(0, 8, 100)}\n",
    "#stack = XGBRegressor()\n",
    "grid_search_stack = RandomizedSearchCV(stack, para_grid, scoring = 'neg_mean_squared_error', n_iter = 250, verbose = 1)\n",
    "grid_search_stack.fit(X_train, target_log)\n",
    "submission.iloc[:,1] = np.floor(np.expm1(grid_search_stack.predict(X_test)))\n",
    "print(submission.head())\n",
    "#submission.to_csv(\"oliver_08281257_stack.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e00e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission.to_csv(\"oliver_08281831_stack.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
